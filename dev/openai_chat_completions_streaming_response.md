# OpenAI Chat Completion Streaming Response Structure

## Overview

OpenAI's chat completion API supports streaming responses using Server-Sent Events (SSE). When `stream: true` is set in the request, the API returns data in chunks rather than waiting for the complete response.

## Stream Format

### Server-Sent Events Structure

Each chunk is sent as a Server-Sent Event with the following format:

```
data: {JSON object}
```

The stream concludes with:

```
data: [DONE]
```

## Complete JSON Response Structure

### Full Chunk Format with All Fields

```json
{
  "id": "chatcmpl-123",
  "object": "chat.completion.chunk",
  "created": 1677652288,
  "model": "gpt-3.5-turbo",
  "system_fingerprint": "fp_44709d6fcb",
  "service_tier": "default",
  "choices": [{
    "index": 0,
    "delta": {
      "role": "assistant",
      "content": "Hello",
      "refusal": null,
      "function_call": {
        "name": "get_weather",
        "arguments": "{\"location\":"
      },
      "tool_calls": [{
        "index": 0,
        "id": "call_abc123",
        "type": "function",
        "function": {
          "name": "get_weather",
          "arguments": "{\"location\":"
        }
      }]
    },
    "logprobs": {
      "content": [{
        "token": "Hello",
        "logprob": -0.31725305,
        "bytes": [72, 101, 108, 108, 111],
        "top_logprobs": [{
          "token": "Hello",
          "logprob": -0.31725305,
          "bytes": [72, 101, 108, 108, 111]
        }]
      }],
      "refusal": null
    },
    "finish_reason": null
  }],
  "usage": {
    "prompt_tokens": 9,
    "completion_tokens": 12,
    "total_tokens": 21,
    "completion_tokens_details": {
      "reasoning_tokens": 0,
      "audio_tokens": 0,
      "accepted_prediction_tokens": 0,
      "rejected_prediction_tokens": 0
    },
    "prompt_tokens_details": {
      "cached_tokens": 0,
      "audio_tokens": 0
    }
  }
}
```

## Detailed Field Descriptions

### Root Level Fields

| Field | Type | Description |
|-------|------|-------------|
| `id` | string | A unique identifier for the chat completion. Each chunk has the same ID |
| `object` | string | The object type, which is always `chat.completion.chunk` |
| `created` | integer | The Unix timestamp (in seconds) of when the chat completion was created. Each chunk has the same timestamp |
| `model` | string | The model used to generate the completion |
| `system_fingerprint` | string | Fingerprint representing the backend configuration that the model runs with |
| `service_tier` | string/null | The processing tier used (`null`, `default`, `scale`, or `flex`) |

### Choices Array

A list of chat completion choices. Can contain more than one element if `n` is greater than 1. Can also be empty for the last chunk if you set `stream_options: {"include_usage": true}`.

| Field | Type | Description |
|-------|------|-------------|
| `index` | integer | The index of the choice in the list of choices |
| `delta` | object | A chat completion delta generated by streamed model responses |
| `finish_reason` | string/null | The reason the model stopped generating tokens |
| `logprobs` | object/null | Log probability information for the choice |

### Delta Object

| Field | Type | Description |
|-------|------|-------------|
| `role` | string | The role of the author (only present in first chunk) |
| `content` | string/null | The contents of the chunk message |
| `refusal` | string/null | The refusal message generated by the model |
| `function_call` | object | **Deprecated** - replaced by `tool_calls` |
| `tool_calls` | array | Tool/function calls made by the model |

#### Function Call Object (Deprecated)

| Field | Type | Description |
|-------|------|-------------|
| `name` | string | The name of the function to call |
| `arguments` | string | The arguments to call the function with (JSON format) |

#### Tool Calls Array

Each tool call object contains:

| Field | Type | Description |
|-------|------|-------------|
| `index` | integer | Index of the tool call |
| `id` | string | Unique identifier for the tool call |
| `type` | string | Type of tool call (e.g., "function") |
| `function` | object | Function details |

Function object contains:
- `name`: string - The name of the function
- `arguments`: string - JSON-formatted arguments (streamed incrementally)

### LogProbs Object

| Field | Type | Description |
|-------|------|-------------|
| `content` | array/null | List of message content tokens with log probability information |
| `refusal` | array/null | List of message refusal tokens with log probability information |

#### Content/Refusal Token Objects

Each token object contains:

| Field | Type | Description |
|-------|------|-------------|
| `token` | string | The token |
| `logprob` | number | The log probability of this token (-9999.0 if very unlikely) |
| `bytes` | array/null | UTF-8 bytes representation of the token |
| `top_logprobs` | array | List of most likely tokens at this position |

### Usage Object

Only included in the final chunk when `stream_options: {"include_usage": true}` is set. **Important**: When usage is included, the final chunk will have an empty `choices` array.

| Field | Type | Description |
|-------|------|-------------|
| `prompt_tokens` | integer | Number of tokens in the prompt |
| `completion_tokens` | integer | Number of tokens in the generated completion |
| `total_tokens` | integer | Total number of tokens used (prompt + completion) |
| `completion_tokens_details` | object | Breakdown of completion tokens |
| `prompt_tokens_details` | object | Breakdown of prompt tokens |

#### Completion Tokens Details

| Field | Type | Description |
|-------|------|-------------|
| `reasoning_tokens` | integer | Tokens generated for reasoning |
| `audio_tokens` | integer | Audio tokens generated by the model |
| `accepted_prediction_tokens` | integer | Predicted output tokens that appeared in completion |
| `rejected_prediction_tokens` | integer | Predicted output tokens that didn't appear in completion |

#### Prompt Tokens Details

| Field | Type | Description |
|-------|------|-------------|
| `cached_tokens` | integer | Cached tokens present in the prompt |
| `audio_tokens` | integer | Audio input tokens present in the prompt |

### Finish Reasons

- `stop`: Model hit a natural stop point or provided stop sequence
- `length`: Maximum number of tokens specified in request was reached
- `content_filter`: Content was omitted due to content filters
- `tool_calls`: Model called a tool
- `function_call`: **Deprecated** - Model called a function

## Streaming Examples

### Basic Content Streaming

#### First Chunk (Role Declaration)
```json
{
  "id": "chatcmpl-123",
  "object": "chat.completion.chunk",
  "created": 1677652288,
  "model": "gpt-3.5-turbo",
  "choices": [{
    "index": 0,
    "delta": {"role": "assistant", "content": ""},
    "finish_reason": null
  }]
}
```

**Note**: The `role` field only appears in this first chunk.

#### Content Chunks
```json
{
  "id": "chatcmpl-123",
  "object": "chat.completion.chunk",
  "created": 1677652288,
  "model": "gpt-3.5-turbo",
  "choices": [{
    "index": 0,
    "delta": {"content": "The weather today"},
    "finish_reason": null
  }]
}
```

**Note**: The `content` field may be `null` or an empty string in some chunks.

### Tool Call Streaming

Tool calls are streamed incrementally with arguments split across chunks:

#### Initial Tool Call
```json
{
  "choices": [{
    "delta": {
      "tool_calls": [{
        "index": 0,
        "id": "call_abc123",
        "type": "function",
        "function": {"name": "get_weather", "arguments": ""}
      }]
    }
  }]
}
```

#### Argument Chunks
```json
{
  "choices": [{
    "delta": {
      "tool_calls": [{
        "index": 0,
        "function": {"arguments": "{\"loc"}
      }]
    }
  }]
}
```

```json
{
  "choices": [{
    "delta": {
      "tool_calls": [{
        "index": 0,
        "function": {"arguments": "ation\": \"San"}
      }]
    }
  }]
}
```

### Final Chunk with Usage

When `stream_options: {"include_usage": true}`:

```json
{
  "id": "chatcmpl-123",
  "object": "chat.completion.chunk",
  "created": 1677652288,
  "model": "gpt-3.5-turbo",
  "choices": [],
  "usage": {
    "prompt_tokens": 9,
    "completion_tokens": 12,
    "total_tokens": 21,
    "completion_tokens_details": {
      "reasoning_tokens": 0,
      "audio_tokens": 0,
      "accepted_prediction_tokens": 0,
      "rejected_prediction_tokens": 0
    },
    "prompt_tokens_details": {
      "cached_tokens": 0,
      "audio_tokens": 0
    }
  }
}
```

### Stream Termination
```
data: [DONE]
```

## Implementation Considerations

### Parsing Tool Calls
- Tool call arguments are streamed incrementally
- Concatenate argument chunks to build complete JSON
- Validate JSON before executing functions
- Model may generate invalid JSON or hallucinate parameters

### Handling LogProbs
- Only available if requested in the API call
- `-9999.0` indicates very unlikely tokens
- `top_logprobs` may have fewer entries than requested
- Bytes arrays useful for multi-token characters

### Service Tiers
- `null`: No specific tier specified
- `default`: Standard pricing and performance
- `scale`: Scale tier processing
- `flex`: Flexible processing option (lower cost, potentially longer latency)

Response `service_tier` may differ from request parameter.

## Error Handling

Errors are sent as Server-Sent Events:

```
data: {
  "error": {
    "message": "Error message description",
    "type": "error_type",
    "param": null,
    "code": "error_code"
  }
}
```

## Best Practices

1. **Connection Management**
   - Implement reconnection logic for network failures
   - Handle connection timeouts appropriately
   - Use keep-alive to maintain connections

2. **Content Assembly**
   - Buffer incomplete lines between chunks
   - Concatenate delta content in order
   - Track tool call indices for multiple calls

3. **State Management**
   - Store message role from first chunk
   - Accumulate content across all chunks
   - Track finish_reason for completion

4. **Error Handling**
   - Parse error responses appropriately
   - Handle malformed JSON gracefully
   - Implement retry logic with backoff

5. **Performance**
   - Process chunks as they arrive
   - Don't wait for complete response
   - Update UI incrementally for better UX
