"""OpenAI streaming response models for structured parsing."""

from typing import List, Optional

from pydantic import BaseModel, Field

from luthien_control.api.openai_chat_completions.datatypes import (
    FunctionCall,
    LogProbs,
    ToolCall,
    Usage,
)


class Delta(BaseModel):
    """Chat completion delta generated by streamed model responses."""

    role: Optional[str] = Field(default=None, description="The role of the author (only present in first chunk)")
    content: Optional[str] = Field(default=None, description="The contents of the chunk message")
    refusal: Optional[str] = Field(default=None, description="The refusal message generated by the model")
    function_call: Optional[FunctionCall] = Field(default=None, description="Deprecated - replaced by tool_calls")
    tool_calls: Optional[List[ToolCall]] = Field(default=None, description="Tool/function calls made by the model")


class StreamingChoice(BaseModel):
    """A streaming chat completion choice."""

    index: int = Field(description="The index of the choice in the list of choices")
    delta: Delta = Field(
        default_factory=Delta, description="A chat completion delta generated by streamed model responses"
    )
    finish_reason: Optional[str] = Field(default=None, description="The reason the model stopped generating tokens")
    logprobs: Optional[LogProbs] = Field(default=None, description="Log probability information for the choice")


class OpenAIStreamChunk(BaseModel):
    """Complete structure of an OpenAI streaming chunk."""

    id: str = Field(description="A unique identifier for the chat completion. Each chunk has the same ID")
    object: str = Field(description="The object type, which is always 'chat.completion.chunk'")
    created: int = Field(description="The Unix timestamp (in seconds) of when the chat completion was created")
    model: str = Field(description="The model used to generate the completion")
    system_fingerprint: Optional[str] = Field(
        default=None, description="Fingerprint representing the backend configuration"
    )
    service_tier: Optional[str] = Field(
        default=None, description="The processing tier used (null, default, scale, or flex)"
    )
    choices: List[StreamingChoice] = Field(default_factory=list, description="A list of chat completion choices")
    usage: Optional[Usage] = Field(
        default=None, description="Token usage statistics (only in final chunk with stream_options)"
    )


class OpenAIStreamError(BaseModel):
    """Error structure in OpenAI streaming responses."""

    message: str = Field(description="Error message description")
    type: str = Field(description="Error type")
    param: Optional[str] = Field(None, description="Parameter that caused the error")
    code: Optional[str] = Field(None, description="Error code")


class OpenAIStreamEvent(BaseModel):
    """Wrapper for either a chunk or an error in the stream."""

    chunk: Optional[OpenAIStreamChunk] = Field(None, description="A streaming chunk if successful")
    error: Optional[OpenAIStreamError] = Field(None, description="Error information if failed")
    raw_data: Optional[str] = Field(None, description="Raw data string for special cases like '[DONE]'")

    @classmethod
    def from_sse_data(cls, data: str) -> "OpenAIStreamEvent":
        """Parse a Server-Sent Event data line into a structured event.

        Args:
            data: The data portion of an SSE event (after 'data: ')

        Returns:
            OpenAIStreamEvent with either chunk, error, or raw_data populated
        """
        import json

        # Handle special case of stream termination
        if data.strip() == "[DONE]":
            return cls(raw_data="[DONE]", chunk=None, error=None)

        # Try to parse as JSON
        try:
            parsed = json.loads(data)

            # Check if it's an error response
            if "error" in parsed:
                return cls(error=OpenAIStreamError(**parsed["error"]), chunk=None, raw_data=None)

            # Otherwise, it should be a chunk
            return cls(chunk=OpenAIStreamChunk(**parsed), error=None, raw_data=None)

        except (json.JSONDecodeError, Exception):
            # If we can't parse it, store as raw data
            return cls(raw_data=data, chunk=None, error=None)

    @property
    def is_done(self) -> bool:
        """Check if this event represents the end of the stream."""
        return self.raw_data == "[DONE]"

    @property
    def is_error(self) -> bool:
        """Check if this event contains an error."""
        return self.error is not None

    @property
    def is_chunk(self) -> bool:
        """Check if this event contains a valid chunk."""
        return self.chunk is not None
